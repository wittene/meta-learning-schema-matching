{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "539ce224",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d53ff41",
   "metadata": {},
   "source": [
    "## Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42cc01bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params(object):\n",
    "    task = 'mrpc'\n",
    "    run_id = 0\n",
    "    batch_size = 64\n",
    "    max_len = 256\n",
    "    lr = 3e-5\n",
    "    n_epochs = 20\n",
    "    logdir = 'checkpoints/'\n",
    "    lm = 'roberta'\n",
    "    bert_path = None\n",
    "    fp16 = False\n",
    "    finetuning = False\n",
    "    da = None\n",
    "    size = None\n",
    "    alpha_aug = 0.8\n",
    "    alpha = 0.2\n",
    "    num_aug = 2\n",
    "    u_lambda = 10.0\n",
    "    no_ssl = False\n",
    "    balance = False\n",
    "    warmup = False\n",
    "    test_file = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f2cc2f",
   "metadata": {},
   "source": [
    "## Set Parameters Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0ee245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper-parameters ##\n",
    "hp = Params()\n",
    "hp.task = 'em_SANTOS-XS_notest'\n",
    "hp.size = 300\n",
    "hp.logdir = 'results_em/'\n",
    "hp.finetuning = True\n",
    "hp.batch_size = 32\n",
    "hp.lr = 3e-5\n",
    "hp.n_epochs = 20\n",
    "hp.max_len = 128\n",
    "hp.fp16 = True\n",
    "hp.lm = 'roberta'\n",
    "hp.da = 'None'\n",
    "hp.balance = True\n",
    "hp.run_id = 0\n",
    "\n",
    "## Directory with test files ##\n",
    "test_dir = 'data/em/SANTOS-XS/individual tests/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7f846c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c96a489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import sklearn.metrics as metrics\n",
    "import uuid\n",
    "import pandas as pd\n",
    "\n",
    "from rotom.dataset import TextCLSDataset\n",
    "from ditto.dataset import DittoDataset\n",
    "from functools import partial\n",
    "from torch.utils import data\n",
    "from snippext.model import MultiTaskNet\n",
    "from snippext.dataset import *\n",
    "from tensorboardX import SummaryWriter\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from apex import amp\n",
    "from transformers.data import glue_processors, glue_compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d5e06",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Copied from `train_any.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73f116d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = {'AMAZON2': 2, 'AMAZON5': 5,\n",
    "               'AG': 4}\n",
    "\n",
    "vocabs = {'SNIPS': ['AddToPlaylist', 'BookRestaurant',\n",
    "                 'GetWeather', 'PlayMusic',\n",
    "                 'RateBook', 'SearchCreativeWork',\n",
    "                 'SearchScreeningEvent'],\n",
    "          'ATIS': ['atis_abbreviation', 'atis_aircraft',\n",
    "              'atis_airfare', 'atis_airline',\n",
    "              'atis_airline#atis_flight_no', 'atis_airport',\n",
    "              'atis_capacity', 'atis_city', 'atis_distance',\n",
    "              'atis_flight', 'atis_flight#atis_airfare',\n",
    "              'atis_flight_no', 'atis_flight_time',\n",
    "              'atis_ground_fare', 'atis_ground_service',\n",
    "              'atis_quantity', 'atis_restriction',\n",
    "              'atis_meal',\n",
    "              'atis_day_name',\n",
    "              'atis_airfare#atis_flight',\n",
    "              'atis_flight#atis_airline',\n",
    "              'atis_flight_no#atis_airline',\n",
    "              'atis_airfare#atis_flight_time',\n",
    "              'atis_ground_service#atis_ground_fare'],\n",
    "          'TREC': ['0', '1', '2', '3', '4', '5'],\n",
    "          'SST-2': ['0', '1'],\n",
    "          'SST-5': ['0', '1', '2', '3', '4'],\n",
    "          'IMDB': ['pos', 'neg']}\n",
    "\n",
    "\n",
    "def get_cls_config(hp):\n",
    "    \"\"\"Get configuration of the task\"\"\"\n",
    "    taskname = hp.task\n",
    "    if 'em_' in taskname:\n",
    "        name = taskname[3:]\n",
    "        vocab = ['0', '1']\n",
    "        path = 'data/em/%s/' % name\n",
    "        config = {'name': taskname,\n",
    "                'task_type': 'classification',\n",
    "                'vocab': vocab}\n",
    "        return config,\\\n",
    "               DittoDataset,\\\n",
    "               DittoDataset\n",
    "    elif 'cleaning_' in taskname:\n",
    "        LL = taskname.split('_')\n",
    "        if hp.size is not None:\n",
    "            size, idx = str(hp.size), str(hp.run_id)\n",
    "            name = LL[1]\n",
    "        else:\n",
    "            prefix, size, idx = LL[0], LL[-2], LL[-1]\n",
    "            name = '_'.join(LL[1:-2])\n",
    "\n",
    "        path = 'data/cleaning/%s/%s_10000/%s/' % (name, size, idx)\n",
    "        vocab = ['0', '1']\n",
    "        config = {'name': taskname,\n",
    "                  'task_type': 'classification',\n",
    "                  'vocab': vocab}\n",
    "        return config, DittoDataset, DittoDataset\n",
    "    elif 'compare' in taskname:\n",
    "        # compare2_SST-2\n",
    "        LL = taskname.split('_')\n",
    "        prefix, name = LL[0], LL[1]\n",
    "        path = 'data/textcls/%s/%s/' % (prefix, name)\n",
    "        vocab = vocabs[name]\n",
    "        idx = str(hp.run_id)\n",
    "        config = {'name': taskname,\n",
    "                  'task_type': 'classification',\n",
    "                  'vocab': vocab}\n",
    "        return config, TextCLSDataset, TextCLSDataset\n",
    "    else:\n",
    "        # Text CLS datasets\n",
    "        if 'textcls_' in taskname:\n",
    "            taskname = taskname.replace('textcls_', '')\n",
    "        if hp.size is None:\n",
    "            path, size = taskname.split('_')\n",
    "        else:\n",
    "            path = taskname\n",
    "            size = str(hp.size)\n",
    "        path = path.upper()\n",
    "        if path in vocabs:\n",
    "            vocab = vocabs[path]\n",
    "        else:\n",
    "            vocab = [str(i) for i in \\\n",
    "                    range(1, num_classes[path]+1)]\n",
    "        path = 'data/textcls/%s' % path\n",
    "\n",
    "        config = {'name': taskname,\n",
    "                  'task_type': 'classification',\n",
    "                  'vocab': vocab}\n",
    "        return config, TextCLSDataset, TextCLSDataset\n",
    "\n",
    "def get_ops(hp):\n",
    "    \"\"\"return a pair of DA operators for each task\"\"\"\n",
    "    em = [\"t5\", \"del\", \"del\"]\n",
    "    cls = [\"t5\", \"token_repl_tfidf\", \"token_del_tfidf\"]\n",
    "    cleaning = [\"t5\", \"swap\", \"swap\"]\n",
    "\n",
    "    if 'cleaning_' in task:\n",
    "        return cleaning\n",
    "    if \"em_\" in task: # EM\n",
    "        return em\n",
    "    else:\n",
    "        return cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a04f25",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a0ca1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(task_config, hp):\n",
    "    \"\"\"Get model and optimizer\n",
    "    Args:\n",
    "        task_config (dictionary): the configuration of the task\n",
    "        hp (Namespace): the parsed hyperparameters\n",
    "        \n",
    "    Returns:\n",
    "        model, optimizer\n",
    "    \"\"\"\n",
    "    # initialize model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model = MultiTaskNet([task_config], device,\n",
    "                         lm=hp.lm, bert_path=hp.bert_path)\n",
    "\n",
    "    # move to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=hp.lr)\n",
    "    if device == 'cuda' and hp.fp16:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O2')\n",
    "\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91504562",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Adapted from `snippext.train_util.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12ffbec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier(model, iterator, threshold=None, get_threshold=False):\n",
    "    \"\"\"Evaluate a classification model state on a dev/test set.\n",
    "\n",
    "    Args:\n",
    "        model (MultiTaskNet): the model state\n",
    "        iterator (DataLoader): a batch iterator of the dev/test set\n",
    "        threshold (float, optional): the cut-off threshold for binary cls\n",
    "        get_threshold (boolean, optional): return the selected threshold if True\n",
    "\n",
    "    Returns:\n",
    "        float: Precision (or accuracy if more than 2 classes)\n",
    "        float: Recall (or accuracy if more than 2 classes)\n",
    "        float: F1 (or macro F1 if more than 2 classes)\n",
    "        float: The Loss\n",
    "        float: The cut-off threshold\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    Y = []\n",
    "    Y_hat = []\n",
    "    Y_prob = []\n",
    "    loss_list = []\n",
    "    total_size = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            _, x, _, _, _, y, _, taskname = batch\n",
    "            taskname = taskname[0]\n",
    "            logits, y1, y_hat = model(x, y, task=taskname)\n",
    "            logits = logits.view(-1, logits.shape[-1])\n",
    "            y1 = y1.view(-1)\n",
    "            if 'sts-b' in taskname.lower():\n",
    "                loss = nn.MSELoss()(logits, y1)\n",
    "            else:\n",
    "                loss = nn.CrossEntropyLoss()(logits, y1)\n",
    "\n",
    "            loss_list.append(loss.item() * y.shape[0])\n",
    "            total_size += y.shape[0]\n",
    "\n",
    "            Y.extend(y.numpy().tolist())\n",
    "            Y_hat.extend(y_hat.cpu().numpy().tolist())\n",
    "            Y_prob.extend(logits.softmax(dim=-1).max(dim=-1)[0].cpu().numpy().tolist())\n",
    "\n",
    "    loss = sum(loss_list) / total_size\n",
    "\n",
    "    print(\"=============%s==================\" % taskname)\n",
    "\n",
    "    # for glue\n",
    "    if taskname in glue_processors:\n",
    "        Y_hat = np.array(Y_hat).squeeze()\n",
    "        Y = np.array(Y)\n",
    "        result = glue_compute_metrics(taskname, Y_hat, Y)\n",
    "        result['loss'] = loss\n",
    "        print(result)\n",
    "        return result\n",
    "    elif taskname[:5] == 'glue_':\n",
    "        task = taskname.split('_')[1].lower()\n",
    "        Y_hat = np.array(Y_hat).squeeze()\n",
    "        Y = np.array(Y)\n",
    "        result = glue_compute_metrics(task, Y_hat, Y)\n",
    "        result['loss'] = loss\n",
    "        print(result)\n",
    "        return result\n",
    "    else:\n",
    "        num_classes = len(set(Y))\n",
    "        # Binary classification\n",
    "        if num_classes <= 2:\n",
    "            accuracy = metrics.accuracy_score(Y, Y_hat)\n",
    "            precision = metrics.precision_score(Y, Y_hat)\n",
    "            recall = metrics.recall_score(Y, Y_hat)\n",
    "            f1 = metrics.f1_score(Y, Y_hat)\n",
    "            if any([prefix in taskname for prefix in \\\n",
    "                ['cleaning_', 'em_']]): # handle imbalance:\n",
    "                max_f1 = f1\n",
    "                if threshold is None:\n",
    "                    for th in np.arange(0.0, 1.0, 0.005):\n",
    "                        Y_hat = [y if p > th else 0 for (y, p) in zip(Y_hat, Y_prob)]\n",
    "                        f1 = metrics.f1_score(Y, Y_hat)\n",
    "                        if f1 > max_f1:\n",
    "                            max_f1 = f1\n",
    "                            accuracy = metrics.accuracy_score(Y, Y_hat)\n",
    "                            precision = metrics.precision_score(Y, Y_hat)\n",
    "                            recall = metrics.recall_score(Y, Y_hat)\n",
    "                            threshold = th\n",
    "                    f1 = max_f1\n",
    "                else:\n",
    "                    Y_hat = [y if p > threshold else 0 for (y, p) in zip(Y_hat, Y_prob)]\n",
    "                    accuracy = metrics.accuracy_score(Y, Y_hat)\n",
    "                    precision = metrics.precision_score(Y, Y_hat)\n",
    "                    recall = metrics.recall_score(Y, Y_hat)\n",
    "                    f1 = metrics.f1_score(Y, Y_hat)\n",
    "\n",
    "            print(\"accuracy=%.3f\"%accuracy)\n",
    "            print(\"precision=%.3f\"%precision)\n",
    "            print(\"recall=%.3f\"%recall)\n",
    "            print(\"f1=%.3f\"%f1)\n",
    "            print(\"======================================\")\n",
    "            if get_threshold:\n",
    "                return accuracy, precision, recall, f1, loss, threshold\n",
    "            else:\n",
    "                return accuracy, precision, recall, f1, loss\n",
    "        else:\n",
    "            accuracy = metrics.accuracy_score(Y, Y_hat)\n",
    "            f1 = metrics.f1_score(Y, Y_hat, average='macro')\n",
    "            precision = recall = accuracy # We might just not return anything\n",
    "            print(\"accuracy=%.3f\"%accuracy)\n",
    "            print(\"macro_f1=%.3f\"%f1)\n",
    "            print(\"======================================\")\n",
    "            return accuracy, f1, loss\n",
    "\n",
    "\n",
    "def eval_on_task(model,\n",
    "                 task,\n",
    "                 test_iter,\n",
    "                 test_dataset):\n",
    "    \"\"\"Run the eval function on the dev/test datasets and log the results.\n",
    "\n",
    "    Args:\n",
    "        model (MultiTaskNet): the model state\n",
    "        task (str): the task name to be evaluated\n",
    "        test_iter (DataLoader): the test set iterator\n",
    "        run_tag (str): the tag of the run\n",
    "\n",
    "    Returns:\n",
    "        test metrics: dict\n",
    "    \"\"\"\n",
    "    t_prec = t_recall = t_f1 = t_loss = None\n",
    "    if any([prefix in task for prefix in \\\n",
    "            ['cleaning_', 'em_']]): # handle imbalance:\n",
    "        print('Test:')\n",
    "        # t_acc, t_prec, t_recall, t_f1, t_loss = eval_classifier(model, test_iter)\n",
    "        t_acc, t_prec, t_recall, t_f1, t_loss, _ = eval_classifier(model, test_iter, get_threshold=True)\n",
    "        scalars = {'acc': t_acc,\n",
    "                   'precision': t_prec,\n",
    "                   'recall': t_recall,\n",
    "                   'f1': t_f1,\n",
    "                   'loss': t_loss}\n",
    "    else:\n",
    "        if test_iter is not None:\n",
    "            print('Test:')\n",
    "            t_output = eval_classifier(model, test_iter)\n",
    "\n",
    "        if len(v_output) == 5:\n",
    "            acc, prec, recall, f1, v_loss = v_output\n",
    "            t_acc, t_prec, t_recall, t_f1, t_loss = t_output\n",
    "            scalars = {'acc': t_acc,\n",
    "                       'precision': t_prec,\n",
    "                       'recall': t_recall,\n",
    "                       'f1': t_f1,\n",
    "                       'loss': t_loss}\n",
    "        else:\n",
    "            acc, f1, v_loss = v_output\n",
    "            t_acc, t_f1, t_loss = t_output\n",
    "            scalars = {'acc': t_acc,\n",
    "                       'f1': t_f1,\n",
    "                       'loss': t_loss}\n",
    "\n",
    "    return scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc446ff",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc054be2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ewitten/rotom/env/lib/python3.8/site-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
      "  warnings.warn(msg, DeprecatedFeatureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed\n",
    "seed = hp.run_id\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# create the tag of the run\n",
    "if hp.no_ssl:\n",
    "    run_tag = '%s_lm=%s_da=%s_no_ssl_alpha=%.1f_id=%d' % (hp.task, hp.lm, hp.da, hp.alpha_aug, hp.run_id)\n",
    "else:\n",
    "    run_tag = '%s_lm=%s_da=%s_alpha=%.1f_id=%d' % (hp.task, hp.lm, hp.da, hp.alpha_aug, hp.run_id)\n",
    "if hp.size is not None:\n",
    "    run_tag += '_size=%d' % hp.size\n",
    "\n",
    "checkpt_path = run_tag + '_dev.pt'\n",
    "\n",
    "config, Dataset, TestDataset = get_cls_config(hp)\n",
    "\n",
    "if hp.balance:\n",
    "    Dataset = partial(Dataset, balance=hp.balance)\n",
    "\n",
    "task = config['name']\n",
    "vocab = config['vocab']\n",
    "task_type = config['task_type']\n",
    "\n",
    "model, optimizer = get_model(config, hp)\n",
    "model.load_state_dict(torch.load(checkpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1914b439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "civic_building_locations.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.909\n",
      "precision=1.000\n",
      "recall=0.750\n",
      "f1=0.857\n",
      "======================================\n",
      "complaint_by_practice.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.300\n",
      "precision=0.300\n",
      "recall=1.000\n",
      "f1=0.462\n",
      "======================================\n",
      "contributors_parties.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.714\n",
      "precision=0.667\n",
      "recall=1.000\n",
      "f1=0.800\n",
      "======================================\n",
      "data_mill.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.636\n",
      "precision=0.500\n",
      "recall=1.000\n",
      "f1=0.667\n",
      "======================================\n",
      "deaths_2012_2018.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.667\n",
      "precision=0.625\n",
      "recall=0.909\n",
      "f1=0.741\n",
      "======================================\n",
      "film_locations_in_san_francisco.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.818\n",
      "precision=0.833\n",
      "recall=0.833\n",
      "f1=0.833\n",
      "======================================\n",
      "311_calls_historic_data.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.867\n",
      "precision=0.875\n",
      "recall=0.875\n",
      "f1=0.875\n",
      "======================================\n",
      "abandoned_wells.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=1.000\n",
      "precision=1.000\n",
      "recall=1.000\n",
      "f1=1.000\n",
      "======================================\n",
      "albums.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.250\n",
      "precision=0.000\n",
      "recall=0.000\n",
      "f1=0.000\n",
      "======================================\n",
      "animal_tag_data.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=1.000\n",
      "precision=1.000\n",
      "recall=1.000\n",
      "f1=1.000\n",
      "======================================\n",
      "biodiversity.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.800\n",
      "precision=1.000\n",
      "recall=0.667\n",
      "f1=0.800\n",
      "======================================\n",
      "business_rates.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.543\n",
      "precision=0.529\n",
      "recall=1.000\n",
      "f1=0.692\n",
      "======================================\n",
      "cdc_nutrition_physical_activity.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.846\n",
      "precision=1.000\n",
      "recall=0.714\n",
      "f1=0.833\n",
      "======================================\n",
      "cihr_co-applicant.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.842\n",
      "precision=0.818\n",
      "recall=0.900\n",
      "f1=0.857\n",
      "======================================\n",
      "psyckes_antipsychotic_polypharm.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.556\n",
      "precision=0.556\n",
      "recall=1.000\n",
      "f1=0.714\n",
      "======================================\n",
      "purchasing_card.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.471\n",
      "precision=0.500\n",
      "recall=0.889\n",
      "f1=0.640\n",
      "======================================\n",
      "report_card_discipline_for_2015.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.647\n",
      "precision=0.600\n",
      "recall=1.000\n",
      "f1=0.750\n",
      "======================================\n",
      "immigration_records.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.818\n",
      "precision=0.750\n",
      "recall=1.000\n",
      "f1=0.857\n",
      "======================================\n",
      "ipopayments.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.867\n",
      "precision=0.800\n",
      "recall=1.000\n",
      "f1=0.889\n",
      "======================================\n",
      "job_pay_scales.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.923\n",
      "precision=1.000\n",
      "recall=0.857\n",
      "f1=0.923\n",
      "======================================\n",
      "lane_description.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=1.000\n",
      "precision=1.000\n",
      "recall=1.000\n",
      "f1=1.000\n",
      "======================================\n",
      "mines.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.714\n",
      "precision=0.750\n",
      "recall=0.750\n",
      "f1=0.750\n",
      "======================================\n",
      "monthly_data_feed.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.286\n",
      "precision=0.286\n",
      "recall=1.000\n",
      "f1=0.444\n",
      "======================================\n",
      "new_york_city_restaurant_inspec.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.526\n",
      "precision=0.526\n",
      "recall=1.000\n",
      "f1=0.690\n",
      "======================================\n",
      "oil_and_gas_summary_production_.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.556\n",
      "precision=0.571\n",
      "recall=0.800\n",
      "f1=0.667\n",
      "======================================\n",
      "practice_reference.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.545\n",
      "precision=0.545\n",
      "recall=1.000\n",
      "f1=0.706\n",
      "======================================\n",
      "prescribing.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.727\n",
      "precision=1.000\n",
      "recall=0.500\n",
      "f1=0.667\n",
      "======================================\n",
      "wholesale_markets.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.615\n",
      "precision=0.583\n",
      "recall=1.000\n",
      "f1=0.737\n",
      "======================================\n",
      "workforce_management_informatio.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.987\n",
      "precision=1.000\n",
      "recall=0.975\n",
      "f1=0.987\n",
      "======================================\n",
      "ydn_spending_data.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.800\n",
      "precision=0.778\n",
      "recall=0.875\n",
      "f1=0.824\n",
      "======================================\n",
      "senior_officials_expenses.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.421\n",
      "precision=0.438\n",
      "recall=0.778\n",
      "f1=0.560\n",
      "======================================\n",
      "stockport_contracts.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.486\n",
      "precision=0.500\n",
      "recall=0.842\n",
      "f1=0.627\n",
      "======================================\n",
      "time_spent_watching_vcr_movies.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.600\n",
      "precision=0.600\n",
      "recall=1.000\n",
      "f1=0.750\n",
      "======================================\n",
      "tuition_assistance_program_tap_.txt\n",
      "Test:\n",
      "=============em_SANTOS-XS_notest==================\n",
      "accuracy=0.556\n",
      "precision=0.556\n",
      "recall=1.000\n",
      "f1=0.714\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "test_files = os.listdir(test_dir)\n",
    "results = []\n",
    "for testset in test_files:\n",
    "    if os.path.isdir(os.path.join(test_dir, testset)): continue\n",
    "    print(testset)\n",
    "    test_dataset = TestDataset(os.path.join(test_dir, testset), vocab, task, lm=hp.lm, max_len=hp.max_len)\n",
    "    padder = SnippextDataset.pad\n",
    "    test_iter = data.DataLoader(dataset=test_dataset,\n",
    "                                 batch_size=hp.batch_size*4,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=0,\n",
    "                                 collate_fn=padder)\n",
    "    scalars = eval_on_task(model,\n",
    "                             task,\n",
    "                             test_iter,\n",
    "                             test_dataset)\n",
    "    scalars[\"dataset\"] = testset[:-4]\n",
    "    results.append(scalars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd9686ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(results)\n",
    "results_df.to_csv(os.path.join(test_dir, 'out', run_tag + '__all_results.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
