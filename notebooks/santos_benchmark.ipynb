{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Overview\n",
        "\n",
        "This notebook generates test, train, and validation data from the SANTOS benchmark. The resulting files are serialized for input to a language model, such as in the Rotom framework."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VrvJDsbJ4Rdq"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-9Y81PqueImd"
      },
      "source": [
        "1. Download the SANTOS labeled benchmark.\n",
        "2. The data lake tables -- in the `datalake` directory -- are the tables to be sampled. Update the `table_dir` variable under the *Cluster Tables* section to point to this directory, relative to this notebook.\n",
        "3. The test, train, and validation sets are written to separate `.txt` files. Update the `test_txt`, `train_txt`, and `valid_txt` variables under the *Open Files* section to the desired output files."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui5rPfMg6ASz"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSlfrJxb6DLb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "from random import sample\n",
        "from pandas.api.types import is_string_dtype"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tJlSVKQY4qJ_"
      },
      "source": [
        "# Cluster Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUGHAO0i1lhS",
        "outputId": "189ad1d0-8199-40f8-fbfe-f40951c73721"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['311_calls_historic_data',\n",
              " 'abandoned_wells',\n",
              " 'albums',\n",
              " 'animal_tag_data',\n",
              " 'biodiversity',\n",
              " 'business_rates',\n",
              " 'cdc_nutrition_physical_activity_and_obesity_legislation',\n",
              " 'cihr_co-applicant',\n",
              " 'civic_building_locations',\n",
              " 'complaint_by_practice',\n",
              " 'contributors_parties',\n",
              " 'data_mill',\n",
              " 'deaths_2012_2018',\n",
              " 'film_locations_in_san_francisco',\n",
              " 'HMRC_exceptions_to_spending_controls_April_to_June_2018',\n",
              " 'HMRC_exceptions_to_spending_controls_April_to_June_2018_facilities',\n",
              " 'HMRC_exceptions_to_spending_controls_April_to_June_2019',\n",
              " 'HMRC_WMI_headcount_and_payroll_data_Mar',\n",
              " 'HMRC_exceptions_to_spending_controls_October_to_December_2017',\n",
              " 'HMRC_Officials_meetings_with_tobacco_stakeholders_Apr_2015_to_Jun',\n",
              " 'HMRC_Officials_meetings_with_tobacco_stakeholders_Apr_2017_to_June',\n",
              " 'HMRC_Officials_meetings_with_tobacco_stakeholders_Apr_2018_to_June',\n",
              " 'HMRC_Officials_meetings_with_tobacco_stakeholders_Jan_2020_to_Mar',\n",
              " 'HMRC_Officials_meetings_with_tobacco_stakeholders_Jul_2014_to_Sept',\n",
              " 'HMRC_Officials_meetings_with_tobacco_stakeholders_Jul_2015_to__Sept',\n",
              " 'HMRC_WMI_headcount_and_payroll_data_May',\n",
              " 'HMRC_WMI_headcount_and_payroll_data_Nov',\n",
              " 'immigration_records',\n",
              " 'ipopayments',\n",
              " 'job_pay_scales',\n",
              " 'lane_description',\n",
              " 'mines',\n",
              " 'minister_meetings',\n",
              " 'prescribing',\n",
              " 'monthly_data_feed',\n",
              " 'new_york_city_restaurant_inspection_results',\n",
              " 'oil_and_gas_summary_production_data_1967_1999',\n",
              " 'practice_reference',\n",
              " 'psyckes_antipsychotic_polypharmacy_quality_indicators_beginning_2012',\n",
              " 'purchasing_card',\n",
              " 'report_card_discipline_for_2015_16',\n",
              " 'senior_officials_expenses',\n",
              " 'stockport_contracts',\n",
              " 'time_spent_watching_vcr_movies',\n",
              " 'tuition_assistance_program_tap_recipients',\n",
              " 'wholesale_markets',\n",
              " 'workforce_management_information',\n",
              " 'ydn_spending_data']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table_dir = \"datalake/\"\n",
        "all_tables = os.listdir(table_dir)\n",
        "table_sets = [ s for s in dict.fromkeys( [ \"_\".join(f.split(\"_\")[:-1]) for f in all_tables ] ) if s ]\n",
        "table_sets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vOSCh8fX79uz"
      },
      "source": [
        "# Open Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "585lQtvq8a4C"
      },
      "outputs": [],
      "source": [
        "test_txt = open(\"exp/test.txt\", \"w\")\n",
        "train_txt = open(\"exp/train.txt\", \"w\")\n",
        "valid_txt = open(\"exp/valid.txt\", \"w\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6UKct0Eu8J0h"
      },
      "source": [
        "# Iterate and Write Test/Train/Validation Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3knQR1RNisJ"
      },
      "outputs": [],
      "source": [
        "def get_splits(pre: str) -> tuple:\n",
        "  '''\n",
        "  Split the tables into test, train, and validation sets.\n",
        "  \n",
        "  Params:\n",
        "  pre: prefix of table set to split\n",
        "\n",
        "  Returns:\n",
        "  table names of test, train, validation sets\n",
        "  '''\n",
        "  s = [ t for t in all_tables if t.startswith(pre) ]\n",
        "  if len(s) < 6:\n",
        "    test = s[:1]\n",
        "    train = s[1:-1]\n",
        "    valid = s[-1:]\n",
        "  if len(s) < 8:\n",
        "    test = s[:2]\n",
        "    train = s[2:-2]\n",
        "    valid = s[-2:]\n",
        "  else:\n",
        "    test = s[:2]\n",
        "    train = s[2:6]\n",
        "    valid = s[-2:]\n",
        "  return test, train, valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xp1EsovOsIO"
      },
      "outputs": [],
      "source": [
        "TOKEN_CT = 15  # MAX TOKENS SERIALIZED PER COLUMN\n",
        "\n",
        "def get_labels(table_lst: list) -> list:\n",
        "  '''\n",
        "  Generate and serialize positive and negative samples for the set of tables.\n",
        "\n",
        "  Params:\n",
        "  table_lst: list of table names\n",
        "\n",
        "  Returns:\n",
        "  list of serialized samples\n",
        "  '''\n",
        "  dfs = [ pd.read_csv(table_dir + csv) for csv in table_lst ]\n",
        "  labeled = []\n",
        "  for i1, i2 in list(combinations(range(len(table_lst)), 2)):\n",
        "    df1: pd.DataFrame = dfs[i1]\n",
        "    df2: pd.DataFrame = dfs[i2]\n",
        "    sample_neg = True\n",
        "    for c1 in df1.columns:\n",
        "      # only consider string columns\n",
        "      if not is_string_dtype(df1[c1]):\n",
        "        continue\n",
        "      # remove nan values\n",
        "      col1 = df1[c1].dropna()\n",
        "      # if col doesn't have enough tokens left, skip\n",
        "      if len(col1) < TOKEN_CT:\n",
        "        continue\n",
        "      for c2 in df2.columns:\n",
        "        # get class label\n",
        "        cls = 1 if c1 == c2 else 0\n",
        "        # skip if got enough negative samples\n",
        "        if not sample_neg and cls == 0:\n",
        "          continue\n",
        "        # only consider string columns\n",
        "        if not is_string_dtype(df2[c2]):\n",
        "          continue\n",
        "        # remove nan values\n",
        "        col2 = df2[c2].dropna()\n",
        "        # if col doesn't have enough tokens left, skip\n",
        "        if len(col2) < TOKEN_CT:\n",
        "          continue\n",
        "        # serialize\n",
        "        toks1 = [ w for l in col1 for w in str(l).split() ]\n",
        "        line1 = \"COL \" + \" \".join(sample(toks1, TOKEN_CT))\n",
        "        toks2 = [ w for l in col2 for w in str(l).split() ]\n",
        "        line2 = \"COL \" + \" \".join(sample(toks2, TOKEN_CT))\n",
        "        labeled.append(f\"{line1}\\t{line2}\\t{cls}\\n\")\n",
        "      sample_neg = False\n",
        "  return labeled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zqssGRKYBuv"
      },
      "outputs": [],
      "source": [
        "def get_cluster_labels(pre: str, table_lst: list) -> list:\n",
        "  '''\n",
        "  Records the table prefix for each sample generated.\n",
        "  Unused currently, but was used to help split up existing labeled files for closer analysis,\n",
        "  without having to regenerate the data sets (due to randomization when choosing tokens).\n",
        "\n",
        "  Params:\n",
        "  pre: table set prefix\n",
        "  table_lst: list of table names\n",
        "\n",
        "  Returns:\n",
        "  list of prefixes, one per sample generated by this set of tables\n",
        "  '''\n",
        "  dfs = [ pd.read_csv(table_dir + csv) for csv in table_lst ]\n",
        "  labeled = []\n",
        "  for i1, i2 in list(combinations(range(len(table_lst)), 2)):\n",
        "    df1: pd.DataFrame = dfs[i1]\n",
        "    df2: pd.DataFrame = dfs[i2]\n",
        "    sample_neg = True\n",
        "    for c1 in df1.columns:\n",
        "      # only consider string columns\n",
        "      if not is_string_dtype(df1[c1]):\n",
        "        continue\n",
        "      # remove nan values\n",
        "      col1 = df1[c1].dropna()\n",
        "      # if col doesn't have enough tokens left, skip\n",
        "      if len(col1) < TOKEN_CT:\n",
        "        continue\n",
        "      for c2 in df2.columns:\n",
        "        # get class label\n",
        "        cls = 1 if c1 == c2 else 0\n",
        "        # skip if got enough negative samples\n",
        "        if not sample_neg and cls == 0:\n",
        "          continue\n",
        "        # only consider string columns\n",
        "        if not is_string_dtype(df2[c2]):\n",
        "          continue\n",
        "        # remove nan values\n",
        "        col2 = df2[c2].dropna()\n",
        "        # if col doesn't have enough tokens left, skip\n",
        "        if len(col2) < TOKEN_CT:\n",
        "          continue\n",
        "        # record cluster\n",
        "        labeled.append(pre + '\\n')\n",
        "      sample_neg = False\n",
        "  return labeled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P6h_BBvOCAd",
        "outputId": "0c66803c-bdc6-4806-818e-a9bd83a51979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampling from 311_calls_historic_data\n",
            "Sampling from abandoned_wells\n",
            "Sampling from albums\n",
            "Sampling from animal_tag_data\n",
            "Sampling from biodiversity\n",
            "Sampling from business_rates\n",
            "Sampling from cdc_nutrition_physical_activity_and_obesity_legislation\n",
            "Sampling from cihr_co-applicant\n",
            "Sampling from civic_building_locations\n",
            "Sampling from complaint_by_practice\n",
            "Sampling from contributors_parties\n",
            "Sampling from data_mill\n",
            "Sampling from deaths_2012_2018\n",
            "Sampling from film_locations_in_san_francisco\n",
            "Sampling from HMRC_exceptions_to_spending_controls_April_to_June_2018\n",
            "Sampling from HMRC_exceptions_to_spending_controls_April_to_June_2018_facilities\n",
            "Sampling from HMRC_exceptions_to_spending_controls_April_to_June_2019\n",
            "Sampling from HMRC_WMI_headcount_and_payroll_data_Mar\n",
            "Sampling from HMRC_exceptions_to_spending_controls_October_to_December_2017\n",
            "Sampling from HMRC_Officials_meetings_with_tobacco_stakeholders_Apr_2015_to_Jun\n",
            "Sampling from HMRC_Officials_meetings_with_tobacco_stakeholders_Apr_2017_to_June\n",
            "Sampling from HMRC_Officials_meetings_with_tobacco_stakeholders_Apr_2018_to_June\n",
            "Sampling from HMRC_Officials_meetings_with_tobacco_stakeholders_Jan_2020_to_Mar\n",
            "Sampling from HMRC_Officials_meetings_with_tobacco_stakeholders_Jul_2014_to_Sept\n",
            "Sampling from HMRC_Officials_meetings_with_tobacco_stakeholders_Jul_2015_to__Sept\n",
            "Sampling from HMRC_WMI_headcount_and_payroll_data_May\n",
            "Sampling from HMRC_WMI_headcount_and_payroll_data_Nov\n",
            "Sampling from immigration_records\n",
            "Sampling from ipopayments\n",
            "Sampling from job_pay_scales\n",
            "Sampling from lane_description\n",
            "Sampling from mines\n",
            "Sampling from minister_meetings\n",
            "Sampling from prescribing\n",
            "Sampling from monthly_data_feed\n",
            "Sampling from new_york_city_restaurant_inspection_results\n",
            "Sampling from oil_and_gas_summary_production_data_1967_1999\n",
            "Sampling from practice_reference\n",
            "Sampling from psyckes_antipsychotic_polypharmacy_quality_indicators_beginning_2012\n",
            "Sampling from purchasing_card\n",
            "Sampling from report_card_discipline_for_2015_16\n",
            "Sampling from senior_officials_expenses\n",
            "Sampling from stockport_contracts\n",
            "Sampling from time_spent_watching_vcr_movies\n",
            "Sampling from tuition_assistance_program_tap_recipients\n",
            "Sampling from wholesale_markets\n",
            "Sampling from workforce_management_information\n",
            "Sampling from ydn_spending_data\n"
          ]
        }
      ],
      "source": [
        "# MAIN LOOP\n",
        "for pre in table_sets:\n",
        "  print(f\"Sampling from {pre}\")\n",
        "  test, train, valid = get_splits(pre)\n",
        "  test_txt.writelines(get_labels(test))\n",
        "  train_txt.writelines(get_labels(train))\n",
        "  valid_txt.writelines(get_labels(valid))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gTTMyQk3NDej"
      },
      "source": [
        "# Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RytFvSUNE8L"
      },
      "outputs": [],
      "source": [
        "test_txt.close()\n",
        "train_txt.close()\n",
        "valid_txt.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Ui5rPfMg6ASz",
        "tJlSVKQY4qJ_"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
